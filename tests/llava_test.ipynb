{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54cf5cfa025e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"./hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb14afa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2557805986.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    torch_dtype=torch.float16\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from llava.model.language_model.llava_llama import LlavaConfig\n",
    "from peft import PeftModel\n",
    "def load_lora_model(lora_model_name, base_model_name, lora_model_path=None, base_model_path=None, load_in_8bit=False, device=\"cuda\", **kwargs):\n",
    "    if lora_model_path is None:\n",
    "        absolute_lora_model_path = lora_model_name\n",
    "    else:\n",
    "        absolute_lora_model_path = os.path.join(lora_model_path, lora_model_name)\n",
    "    if base_model_path is None:\n",
    "        absolute_base_model_path = base_model_name\n",
    "    else:\n",
    "        absolute_base_model_path = os.path.join(base_model_path, base_model_name)\n",
    "\n",
    "    lora_cfg_pretrained = LlavaConfig.from_pretrained(absolute_lora_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(absolute_base_model_path, use_fast=False)\n",
    "    model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "        model_base,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        device=\"Cuda\",\n",
    "        torch_dtype=torch.float16\n",
    "        config=lora_cfg_pretrained\n",
    "    )\n",
    "    token_num = model.lm_head.out_features\n",
    "    token_dim = model.lm_head.in_features\n",
    "    \n",
    "    if lora_model_path is None:\n",
    "        # download from hf hub\n",
    "        cache_file = hf_hub_download(\n",
    "            lora_model_name,\n",
    "            cache_dir=hf_cache_path,\n",
    "            force_download=True\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908d3d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizer(name_or_path='liuhaotian/llava-v1.5-7b', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50df2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bafc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaLlamaForCausalLM(\n",
       "  (model): LlavaLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (vision_tower): CLIPVisionTower(\n",
       "      (vision_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(577, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mm_projector): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_lora_model(lora_model_name, base_model_name, lora_model_path=None, base_model_path=None, load_in_8bit=False, device=\"cuda\", **kwargs):\n",
    "    if lora_model_path is None:\n",
    "        absolute_lora_model_path = lora_model_name\n",
    "    else:\n",
    "        absolute_lora_model_path = os.path.join(lora_model_path, lora_model_name)\n",
    "    if base_model_path is None:\n",
    "        absolute_base_model_path = base_model_name\n",
    "    else:\n",
    "        absolute_base_model_path = os.path.join(base_model_path, base_model_name)\n",
    "\n",
    "    from llava.model.language_model.llava_llama import LlavaConfig\n",
    "    lora_cfg_pretrained = LlavaConfig.from_pretrained(absolute_lora_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(, use_fast=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b762a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
