{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_DATASETS_CACHE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./hf_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./hf_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"./hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a1e8d3f1b349579e807962f3444716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/slayman/pi/gerstein/xt86/billq/cpsc477_project/cpsc477_billq/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import process_images\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"https://storage.lilbillbiscuit.com/IMG_2184.jpeg\"\n",
    "image = load_image(image) # note this is a PIL image\n",
    "image_size = image.size\n",
    "image_tensor = process_images([image], image_processor, model.config) # \n",
    "if type(image_tensor) is list:\n",
    "    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n",
    "else:\n",
    "conv    image_tensor = image_tensor.to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_conv_mode(model_name):\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "    return conv_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"Can you describe this image?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "conv = conv_templates[infer_conv_mode(model_path)].copy()\n",
    "\n",
    "if image is not None:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "    else:\n",
    "        inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    image = None\n",
    "else:\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/slayman/pi/gerstein/xt86/billq/cpsc477_project/cpsc477_billq/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The image features a wooden door with a large wooden frame, giving it a rustic appearance. The door is open, revealing a hallway with a wooden wall on the other side. The hallway is adorned with a wooden panel, adding to the overall rustic theme.\n",
      "\n",
      "In the background, there is a chair placed against the wall, and a dining table can be seen further back. A person is standing near the table, possibly enjoying a meal or engaging in conversation. The scene creates a warm and inviting atmosphere.</s>\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "      input_ids, \n",
    "      images = image_tensor,\n",
    "      image_sizes = [image_size],\n",
    "      temperature = 0.7,\n",
    "      max_new_tokens = 2048,\n",
    "      use_cache = True\n",
    "    )\n",
    "outputs = tokenizer.decode(output_ids[0]).strip()\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
