{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = \"./hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cs477-final-project/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/ubuntu/cs477-final-project/.conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:14<00:00, 37.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    load_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import process_images\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"https://patentimages.storage.googleapis.com/23/82/97/d28e8e17f1597a/US20140002137A1-20140102-D00002.png\"\n",
    "image = load_image(image) # note this is a PIL image\n",
    "image_size = image.size\n",
    "image_tensor = process_images([image], image_processor, model.config) # \n",
    "if type(image_tensor) is list:\n",
    "    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n",
    "else:\n",
    "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_conv_mode(model_name):\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "    return conv_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"Use this image to classify the patent into a class in the International Patent Classification system. Answer with the a 4-character IPC class, do not answer with anything else.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "conv = conv_templates[infer_conv_mode(model_path)].copy()\n",
    "\n",
    "if image is not None:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "    else:\n",
    "        inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    image = None\n",
    "else:\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cs477-final-project/.conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> G06F19/00</s>\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "      input_ids, \n",
    "      images = image_tensor,\n",
    "      image_sizes = [image_size],\n",
    "      temperature = 0.7,\n",
    "      max_new_tokens = 2048,\n",
    "      use_cache = True\n",
    "    )\n",
    "outputs = tokenizer.decode(output_ids[0]).strip()\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
